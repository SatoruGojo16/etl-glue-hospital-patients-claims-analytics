{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "tags": []
   },
   "source": [
    "# Hospital Patient Claims\n",
    "## Staging to Curated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Run this cell to set up and start your interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%idle_timeout 10\n",
    "%glue_version 5.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "from awsglue import DynamicFrame\n",
    "import json \n",
    "from datetime import date\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "args = getResolvedOptions(sys.argv, ['is_init_load'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa576fdf-bf9b-4770-9b7c-6354878c8691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "glueContext=GlueContext(sc)\n",
    "spark=glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    secret_name = \"dev/motor_theft_vehicles/redshift_connection\"\n",
    "    region_name = \"us-east-1\"\n",
    "    client = boto3.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "    db_config = get_secret_value_response['SecretString']\n",
    "    return db_config\n",
    "\n",
    "db_config = json.loads(get_secret())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conn_options = {\n",
    "    \"url\": db_config['dev_url'],\n",
    "    \"user\": db_config['dev_username'],\n",
    "    \"password\": db_config['dev_password'],\n",
    "    \"redshiftTmpDir\": db_config['dev_redshift_temp_directory'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_load = args['is_init_load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "import awsglue.transforms as  T\n",
    "import  pyspark.sql.functions as  F\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.utils import *\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import sys  \n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext=GlueContext(sc)\n",
    "spark=glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/staging_csv/staging_data_patients_claims.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'claim_id','claim_status','claim_request_amount','claim_initialized_date','claim_rejected_reason','source_file_name','source_file_path','load_timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental load table \n",
    "dyf_incremental_load = glueContext.create_data_frame.from_catalog(\n",
    "    connection_type = 'redshift',\n",
    "    connection_options = my_conn_options,\n",
    "    database_name = 'db_patient_claims_staging',\n",
    "    table_name = 'staging_claims_incremental_load'\n",
    ")\n",
    "\n",
    "# claim table \n",
    "dyf_staging_claims = dyf_incremental_load.select('claim_id','claim_status','claim_request_amount','claim_initialized_date','claim_rejected_reason','source_file_name','source_file_path','load_timestamp')\n",
    "\n",
    "# patient table \n",
    "dyf_staging_patient = dyf_incremental_load.select('patient_id','name_prefix','first_name','last_name','date_of_birth','phone_number','email_id')\n",
    "\n",
    "# policy table \n",
    "dyf_staging_policy = dyf_incremental_load.select('policy_id','policy_start_date','policy_end_date','preimum_amount','coverage_limit')\n",
    "\n",
    "# address table \n",
    "dyf_staging_address = dyf_incremental_load.select('address_id','address_line_1','address_line_2','city','state','zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_merge_query_post_action =\"\"\"\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "UPDATE dim_claims \n",
    "SET is_current = 'N', effective_end_date = getdate()\n",
    "FROM staging_claims\n",
    "WHERE staging_claims.claim_id = dim_claims.claim_id\n",
    "and dim_claims.is_current = 'Y'\n",
    ";\n",
    "\n",
    "INSERT INTO dim_claims(claim_id,claim_status,claim_request_amount,claim_initialized_date,claim_rejected_reason,source_file_name,source_file_path,load_timestamp)\n",
    "SELECT claim_id,claim_status,claim_request_amount,claim_initialized_date,claim_rejected_reason,source_file_name,source_file_path,load_timestamp\n",
    "FROM staging_claims;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "# loading Staging Claims\n",
    "my_conn_options['dbtable'] = \"staging_claims\"\n",
    "my_conn_options['preactions'] = \"TRUNCATE \"+my_conn_options['dbtable']\n",
    "my_conn_options['postactions'] = claims_merge_query_post_action \n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_claims,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_merge_query_post_action =\"\"\"\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "UPDATE dim_patients \n",
    "SET is_current = 'N', effective_end_date = getdate()\n",
    "FROM staging_patients\n",
    "WHERE staging_patients.patient_id = dim_patients.patient_id\n",
    "and dim_patients.is_current = 'Y'\n",
    ";\n",
    "\n",
    "INSERT INTO dim_patients(patient_id,name_prefix,first_name,last_name,date_of_birth,phone_number,email_id)\n",
    "SELECT patient_id,name_prefix,first_name,last_name,date_of_birth,phone_number,email_id\n",
    "FROM staging_patients;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "# loading Stagting Patients\n",
    "my_conn_options['dbtable'] = \"staging_patients\"\n",
    "my_conn_options['preactions'] = \"TRUNCATE \"+my_conn_options['dbtable']\n",
    "my_conn_options['postactions'] = claims_merge_query_post_action \n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_claims,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_merge_query_post_action =\"\"\"\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "UPDATE dim_policy \n",
    "SET is_current = 'N', effective_end_date = getdate()\n",
    "FROM staging_policy\n",
    "WHERE staging_policy.policy_id = dim_policy.policy_id\n",
    "and dim_policy.is_current = 'Y'\n",
    ";\n",
    "\n",
    "INSERT INTO dim_policy(policy_id,policy_start_date,policy_end_date,preimum_amount,coverage_limit)\n",
    "SELECT policy_id,policy_start_date,policy_end_date,preimum_amount,coverage_limit\n",
    "FROM staging_policy;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "# loading Staging Policy\n",
    "my_conn_options['dbtable'] = \"staging_policy\"\n",
    "my_conn_options['preactions'] = \"TRUNCATE \"+my_conn_options['dbtable']\n",
    "my_conn_options['postactions'] = policy_merge_query_post_action \n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_claims,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_merge_query_post_action =\"\"\"\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "UPDATE dim_address \n",
    "SET is_current = 'N', effective_end_date = getdate()\n",
    "FROM staging_address\n",
    "WHERE staging_address.address_id = dim_policy.address_id\n",
    "and dim_address.is_current = 'Y'\n",
    ";\n",
    "\n",
    "INSERT INTO dim_address(address_id,address_line_1,address_line_2,city,state,zip_code)\n",
    "SELECT address_id,address_line_1,address_line_2,city,state,zip_code\n",
    "FROM staging_address;\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "# loading Staging Address\n",
    "my_conn_options['dbtable'] = \"staging_address\"\n",
    "my_conn_options['preactions'] = \"TRUNCATE \"+my_conn_options['dbtable']\n",
    "my_conn_options['postactions'] = address_merge_query_post_action \n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_claims,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "490327ae-149c-4f76-9ae7-22bf76d83c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if initial_load:\n",
    "    dim_date = spark.sql('''\n",
    "    SELECT explode(sequence(to_date('2000-01-01'), to_date('2030-01-01'), interval 1 months)) as date\n",
    "    ''')\n",
    "    dim_date_cols = {\n",
    "    \"date_id\":date_format(dim_date.date,'MMdd'),\n",
    "    \"month\":date_format(dim_date.date, 'M'),\n",
    "    \"month_short\":date_format(dim_date.date, \"LLL\"), \n",
    "    \"month_long\":date_format(dim_date.date, \"LLLL\"),\n",
    "    \"year_short\":date_format(dim_date.date, 'yy'),\n",
    "    \"year_long\":date_format(dim_date.date, 'yyyy'),\n",
    "    \"quarter\":ceil(date_format(dim_date.date, 'M')/3),\n",
    "    }\n",
    "    dim_date = dim_date.withColumns(dim_date_cols)\n",
    "    dim_date = dim_date.drop('date')\n",
    "    dfy_dim_date = glueContext.fromDF(dim_date)\n",
    "    # loading Curated Dim Date\n",
    "    my_conn_options['dbtable'] = \"dim_date\"\n",
    "    glueContext.write_dynamic_frame.from_options(\n",
    "        connection_type=\"redshift\",\n",
    "        connection_options = my_conn_options,\n",
    "        frame = dyf_make_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table_insert_query_post_action =\"\"\"\n",
    "\n",
    "INSERT INTO staging_thefts_load(fact_claim_id,patient_id,address_id,policy_ref_id,claim_id,load_datetime)\n",
    "(\n",
    "SELECT \n",
    "dim_patient.patient_ref_id,\n",
    "dim_claims.claims_ref_id,\n",
    "dim_address.address_ref_id,\n",
    "dim_policy.policy_ref_id,\n",
    "getdate() load_datetime\n",
    "FROM staging_claims_incremental_load\n",
    "join dim_patient on staging_claims_incremental_load.patient_id = dim_patient.patient_id and is_current = 'Y' and effective_end_date is NULL \n",
    "join dim_claims on staging_claims_incremental_load.claim_id = dim_claims.claim_id and is_current = 'Y' and effective_end_date is NULL \n",
    "join dim_address on staging_claims_incremental_load.address_id = dim_address.address_id and is_current = 'Y' and effective_end_date is NULL \n",
    "join dim_policy on staging_claims_incremental_load.policy_id = dim_policy.policy_id and is_current = 'Y' and effective_end_date is NULL \n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# loading Staging Claims INcremental Load \n",
    "dyf_staging_incidents = DynamicFrame.fromDF(staging_incidents, glueContext, 'sm')\n",
    "my_conn_options['dbtable'] = \"staging_claims_incremental_load\"\n",
    "my_conn_options['postactions'] = fact_table_insert_query_post_action\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_incidents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "staging_to_curated",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
