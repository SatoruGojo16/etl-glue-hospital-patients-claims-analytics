{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Hospital Patient Claims\n",
    "## Raw to Staging\n",
    "##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %idle_timeout 10\n",
    "# %glue_version 5.0\n",
    "# %worker_type G.1X\n",
    "# %number_of_workers 2\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import * \n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import random \n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "todays_date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f's3://hospital-patients-claims-bucket/raw_zone/raw_hospital_patients_claims'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = glueContext.create_data_frame.from_options(connection_type='s3', connection_options={\"paths\": [s3_path]}, format='parquet'\n",
    "                                                ,push_down_push_down_predicate= f\"load_date = '{todays_date}'\") \n",
    "df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/raw_csv/raw_data_patients_claims.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.trimming the Columns along with DataType defining\n",
    "df = df.withColumn('partition_key', F.trim(F.col('partition_key').cast(StringType()))) \\\n",
    "              .withColumn('patient_id', F.trim(F.col('patient_id').cast(StringType()))) \\\n",
    "              .withColumn('name_prefix', F.trim(F.col('name_prefix').cast(StringType()))) \\\n",
    "              .withColumn('first_name', F.trim(F.col('first_name').cast(StringType()))) \\\n",
    "              .withColumn('last_name', F.trim(F.col('last_name').cast(StringType()))) \\\n",
    "              .withColumn('date_of_birth', F.trim(F.col('date_of_birth').cast(StringType()))) \\\n",
    "              .withColumn('phone_number', F.trim(F.col('phone_number').cast(StringType()))) \\\n",
    "              .withColumn('email_id', F.trim(F.col('email_id').cast(StringType()))) \\\n",
    "              .withColumn('policy_id', F.trim(F.col('policy_id').cast(StringType()))) \\\n",
    "              .withColumn('policy_start_date', F.trim(F.col('policy_start_date').cast(StringType()))) \\\n",
    "              .withColumn('policy_end_date', F.trim(F.col('policy_end_date').cast(StringType()))) \\\n",
    "              .withColumn('preimum_amount', F.trim(F.col('preimum_amount').cast(StringType()))) \\\n",
    "              .withColumn('coverage_limit', F.trim(F.col('coverage_limit').cast(StringType()))) \\\n",
    "              .withColumn('addres_id', F.trim(F.col('addres_id').cast(StringType()))) \\\n",
    "              .withColumn('addressline', F.trim(F.col('addressline').cast(StringType()))) \\\n",
    "              .withColumn('borough', F.trim(F.col('borough').cast(StringType()))) \\ # Borough\n",
    "              .withColumn('city', F.trim(F.col('city').cast(StringType()))) \\ # London\n",
    "              .withColumn('state', F.trim(F.col('state').cast(StringType()))) \\ # London\n",
    "              .withColumn('claim_id', F.trim(F.col('claim_id').cast(StringType()))) \\\n",
    "              .withColumn('claim_initialized_date', F.trim(F.col('claim_initialized_date').cast(StringType()))) \\\n",
    "              .withColumn('claim_request_amount', F.trim(F.col('claim_request_amount').cast(StringType()))) \\\n",
    "              .withColumn('load_timestamp', F.trim(F.col('load_timestamp').cast(StringType()))) \\\n",
    "              .withColumn('source_file_name', F.trim(F.col('source_file_name').cast(StringType()))) \\\n",
    "              .withColumn('source_file_path', F.trim(F.col('source_file_path').cast(StringType()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Unwanted Records\n",
    "df = df.drop_duplicates()\n",
    "# FIltering by NOT NULL Values on required ID Values\n",
    "df = df.filter(F.col('Patient_id').isNotNull() & F.col('policy_id').isNotNull() & F.col('addres_id').isNotNull() & F.col('claim_id').isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate High Risk Claims and add it as a Flag Indicator \n",
    "df = df.withColumn('high_risk_claim_flag', F.when(((df.claim_request_amount / df.coverage_limit) * 100) > 80, 'Y').otherwise('N'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejected Claims with Rejected Reasons \n",
    "df = df.filter(df.claim_status == 'Rejected' & df.claim_reject_reason.isNULL() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Enrichment of Borough Details Values\n",
    "df_borough_details = spark.read.csv('data_london_borough_details.csv')\n",
    "\n",
    "# Enrich DataFrame with borough abbrevation of London \n",
    "combined_df = df.join(F.broadcast(df_borough_details), df.borough = df.borough, 'left')\n",
    "df = df.withColumn('borough_abbrev', combined_df.borough_abbrev)\n",
    "\t\n",
    "# Enrich DataFrame with borough code of London \n",
    "df = df.withColumn('borough_code', combined_df.borough_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all date-related columns to a standard date/timestamp type\n",
    "df = df.withColumn('date_of_birth', F.to_date(df.date_of_birth, 'dd-MM-yyyy'))\n",
    "df = df.withColumn('policy_start_date', F.to_date(df.policy_start_date,'dd/MM/yyyy'))\n",
    "df = df.withColumn('policy_end_date', F.to_date(df.policy_end_date, 'yyyy/MM/dd'))\n",
    "\n",
    "# Policy End date should be greater than start date \n",
    "df = df.where(df.policy_end_date < df.policy_start_date).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Amount Column to be having Only Numbers - No Negative\n",
    "df = df.withColumn('claim_request_amount', F.regexp_replace('claim_request_amount','\\$',''))\n",
    "df = df.withColumn('preimum_amount', F.regexp_replace('preimum_amount','\\$',''))\n",
    "df = df.withColumn('coverage_limit', F.regexp_replace('coverage_limit','\\$',''))\n",
    "\n",
    "# Amount Column Format enforce\n",
    "df = df.where(df.claim_request_amount > 0)\n",
    "df = df.where(df.preimum_amount > 0)\n",
    "df = df.where(df.coverage_limit > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL Caps on Name Fields\n",
    "df = df.withColumn('first_name', F.initcap(F.col('first_name')))\n",
    "df = df.withColumn('last_name', F.initcap(F.col('last_name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Name Column to be having Only Letters\n",
    "df.withColumn('first_name', regexp_replace(F.col('first_name'), '[^a-zA-Z]', ''))\n",
    "df.withColumn('last_name', regexp_replace(F.col('last_name'), '[^a-zA-Z]', ''))\n",
    "df.withColumn('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check email format\n",
    "df = df.filter(F.regexp_like(df.email_id, F.lit(r'^[a-z0-9_.+-]+@[a-z]+\\.[a-z]+$')) == True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ph Number\n",
    "df = df.filter(F.regexp_like('phone_number',F.lit(r'^[0-9]{10}$')) == True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('data/staging_csv/staging_data_patients_claims.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    secret_name = \"dev/motor_theft_vehicles/redshift_connection\"\n",
    "    region_name = \"us-east-1\"\n",
    "    client = boto3.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "    db_config = get_secret_value_response['SecretString']\n",
    "    return db_config\n",
    "\n",
    "db_config = json.loads(get_secret())\n",
    "my_conn_options = {\n",
    "    \"url\": db_config['dev_url'],\n",
    "    \"user\": db_config['dev_username'],\n",
    "    \"password\": db_config['dev_password'],\n",
    "    \"redshiftTmpDir\": db_config['dev_redshift_temp_directory'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyf = DynamicFrame.fromDF(df)\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    connection_type=\"redshift\",\n",
    "    connection_options = my_conn_options,\n",
    "    frame = dyf_staging_claims,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
